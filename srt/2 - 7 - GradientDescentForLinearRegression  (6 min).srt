1
00:00:00,454 --> 00:00:02,208
In previous videos, we talked
在以前的视频中我们谈到
(字幕整理：中国海洋大学 黄海广,haiguang2000@qq.com )

2
00:00:02,238 --> 00:00:04,581
about the gradient descent algorithm
关于梯度下降算法

3
00:00:04,581 --> 00:00:06,005
and talked about the linear
梯度下降是很常用的算法
它不仅被用在线性回归上

4
00:00:06,005 --> 00:00:09,071
regression model and the squared error cost function.
和线性回归模型、平方差成本函数

5
00:00:09,071 --> 00:00:10,822
In this video, we're going to
在这段视频中.我们要

6
00:00:10,822 --> 00:00:12,695
put together gradient descent with
将梯度下降

7
00:00:12,695 --> 00:00:14,672
our cost function, and that
和成本函数结合

8
00:00:14,672 --> 00:00:16,652
will give us an algorithm for
在后面的视频中 我们将用到此算法
并将其应用于具体的

9
00:00:16,652 --> 00:00:19,431
linear regression for fitting a straight line to our data.
得到拟合直线的线性回归算法

10
00:00:21,001 --> 00:00:22,743
So, this is
所以这就是

11
00:00:22,743 --> 00:00:25,077
what we worked out in the previous videos.
我们在之前的影片中所做的工作

12
00:00:25,077 --> 00:00:27,095
That's our gradient descent algorithm, which
这是梯度下降法.

13
00:00:27,095 --> 00:00:29,197
should be familiar, and you
这你应该很熟悉

14
00:00:29,197 --> 00:00:31,199
see the linear linear regression model
这就是线性回归模型

15
00:00:31,199 --> 00:00:36,461
with our linear hypothesis and our squared error cost function.
线性假设和平方差成本函数。

16
00:00:36,461 --> 00:00:38,612
What we're going to do is apply
我们将要做的就是

17
00:00:38,612 --> 00:00:42,288
gradient descent to minimize
用梯度下降的方法

18
00:00:44,519 --> 00:00:47,537
our squared error cost function.
最小化平方差成本函数

19
00:00:47,891 --> 00:00:49,381
Now, in order to apply
为了

20
00:00:49,381 --> 00:00:50,896
gradient descent, in order
梯度下降.为了

21
00:00:50,896 --> 00:00:52,695
to write this piece of
这篇文章所写的

22
00:00:52,695 --> 00:00:54,191
code, the key term
代码.关键术语

23
00:00:54,191 --> 00:00:58,384
we need is this derivative term over here.
我们需要的是这个微分名词在这里。

24
00:00:59,692 --> 00:01:00,798
So, we need to figure out
所以.我们需要弄清楚

25
00:01:00,798 --> 00:01:02,830
what is this partial derivative term,
这是什么偏导数项.

26
00:01:02,830 --> 00:01:04,478
and plug in the
并插上

27
00:01:04,478 --> 00:01:06,249
definition of the cost
成本的定义

28
00:01:06,249 --> 00:01:08,418
function J, this turns
函数J.轮流

29
00:01:08,418 --> 00:01:11,074
out to be this "inaudible"
事情是这样的“难以弄清的”

30
00:01:12,613 --> 00:01:15,156
equals sum 1 through M of
通过并购.相等于1的

31
00:01:15,156 --> 00:01:18,856
this squared error
这个误差平方

32
00:01:20,456 --> 00:01:22,023
cost function term, and all
代价函数区间.以及所有

33
00:01:22,023 --> 00:01:23,794
I did here was I just
我没在这里.我只是

34
00:01:23,794 --> 00:01:25,538
you know plugged in the definition of
你知道插入的定义中

35
00:01:25,538 --> 00:01:28,275
the cost function there, and simplifying
并定义的成本函数

36
00:01:28,275 --> 00:01:30,563
little bit more, this turns
有点多.这将

37
00:01:30,563 --> 00:01:34,136
out to be equal to, this
是等于.这

38
00:01:34,136 --> 00:01:36,983
"inaudible" equals sum 1 through M
“难以弄清”通过并购总和等于1

39
00:01:36,983 --> 00:01:40,609
of tetha zero plus tetha one, XI
加tetha tetha零一.XI

40
00:01:41,163 --> 00:01:43,427
minus YI squared.
减YI的平方。

41
00:01:43,427 --> 00:01:44,777
And all I did there was took
这就是我所做的一切了

42
00:01:44,777 --> 00:01:46,983
the definition for my hypothesis
我的假设的定义

43
00:01:46,983 --> 00:01:49,376
and plug that in there.
和插在那里。

44
00:01:49,622 --> 00:01:51,669
And it turns out we need
原来我们需要

45
00:01:51,669 --> 00:01:52,523
to figure out what is
弄清楚什么是

46
00:01:52,523 --> 00:01:54,011
the partial derivative of two
2的偏导数

47
00:01:54,011 --> 00:01:55,284
cases for J equals
案件的J等于

48
00:01:55,284 --> 00:01:57,006
0 and for J equals 1 want
0的J等于1想要

49
00:01:57,006 --> 00:01:58,547
to figure out what is this
要弄清楚这是什么

50
00:01:58,547 --> 00:02:00,767
partial derivative for both the
偏导数同时为

51
00:02:00,767 --> 00:02:04,115
theta(0) case and the theta(1) case.
θ（0）的情况下.θ（1）的情况下。

52
00:02:04,115 --> 00:02:07,012
And I'm just going to write out the answers.
我只是写出来的答案。

53
00:02:07,012 --> 00:02:10,996
It turns out this first term simplifies
事实证明.这第一项简化

54
00:02:10,996 --> 00:02:14,218
to 1/M, sum over
到1 / M.求和

55
00:02:14,218 --> 00:02:16,446
my training set of just
只是我的训练

56
00:02:16,446 --> 00:02:21,146
that, X(i)-  Y(i).
.X（I） -  Y（I）。

57
00:02:21,146 --> 00:02:23,951
And for this term, partial derivative
而这个词.偏导数

58
00:02:23,951 --> 00:02:26,186
with respect to theta(1), it turns
相对于θ（1）.它打开

59
00:02:26,186 --> 00:02:34,954
out I get this term: -Y(i)<i>X(i).</i>
我得到了这个词：-Y（I）<I> X（I）</ I>

60
00:02:35,031 --> 00:02:36,187
Okay.
好吧。

61
00:02:36,402 --> 00:02:38,690
And computing these partial
并计算这些局部

62
00:02:38,690 --> 00:02:40,761
derivatives, so going from
微分工具.所以从

63
00:02:40,761 --> 00:02:43,406
this equation to either
此方程为

64
00:02:43,406 --> 00:02:46,414
of these equations down there, computing
出现了下滑.这些方程计算

65
00:02:46,414 --> 00:02:51,090
those partial derivative terms requires some multivariate calculus.
这些偏导数项要求一些多元演算。

66
00:02:51,090 --> 00:02:53,118
If you know calculus, feel free
如果你知道微积分.随意

67
00:02:53,118 --> 00:02:54,825
to work through the derivations yourself
工作.通过自己的推导

68
00:02:54,825 --> 00:02:57,060
and check take the derivatives
并检查采取的微分工具

69
00:02:57,060 --> 00:02:59,853
you actually get the answers that I got.
你实际上得到.我得到的答案。

70
00:02:59,853 --> 00:03:00,855
But if you are less
但如果你是少

71
00:03:00,855 --> 00:03:02,611
familiar with calculus you don't
熟悉微积分.你不这样做

72
00:03:02,611 --> 00:03:04,235
worry about it, and it
担心它.它

73
00:03:04,235 --> 00:03:06,260
is fine to just take these equations
罚款只是采取这些方程

74
00:03:06,260 --> 00:03:08,025
worked out, and you
制定出来的.和你

75
00:03:08,025 --> 00:03:09,462
won't need to know calculus or
不需要知道微积分或

76
00:03:09,462 --> 00:03:10,340
anything like that in order to
类似的东西.以

77
00:03:10,340 --> 00:03:14,551
do the homework, so to implement gradient descent you'd get that to work.
做功课.所以你会得到那个工作实施梯度下降。

78
00:03:14,551 --> 00:03:16,520
But so, after these definitions,
但是.这些定义之后.

79
00:03:16,520 --> 00:03:18,156
or after what we've worked
或者我们工作后.

80
00:03:18,156 --> 00:03:19,993
out to be the derivatives, which
是的微分物.

81
00:03:19,993 --> 00:03:21,316
is really just the slope of
真的只是斜率

82
00:03:21,316 --> 00:03:22,889
the cost function J.  We
的成本函数J.

83
00:03:22,889 --> 00:03:24,724
can now plug them back into
现在可以将它们放回

84
00:03:24,724 --> 00:03:27,157
our gradient descent algorithm.
我们的梯度下降算法。

85
00:03:27,157 --> 00:03:28,794
So here's gradient descent, or
所以这里的梯度下降.或

86
00:03:28,794 --> 00:03:30,309
the regression, which is going
回归.这是会

87
00:03:30,309 --> 00:03:32,971
to repeat until convergence, theta 0
重复.直到收敛.θ0

88
00:03:32,971 --> 00:03:35,552
and theta one get updated as,
和theta得到更新.

89
00:03:35,552 --> 00:03:37,163
you know, the same minus alpha
你知道.相同的负阿尔法

90
00:03:37,163 --> 00:03:39,591
times the derivative term.
次微分项。

91
00:03:39,591 --> 00:03:43,202
So, this term here.
所以.这个词在这里。

92
00:03:43,202 --> 00:03:46,854
So, here's our linear regression algorithm.
所以.这里是我们的线性回归算法。

93
00:03:46,854 --> 00:03:52,696
This first term here that
这第一项.在这里.

94
00:03:52,696 --> 00:03:54,495
term is, of course, just
长期.当然.只是

95
00:03:54,495 --> 00:03:56,071
a partial derivative of respective
部分微分物的各自的

96
00:03:56,071 --> 00:03:59,995
theta zero, that we worked on in the previous slide.
西塔零.我们曾在上一张幻灯片。

97
00:03:59,995 --> 00:04:03,454
And this second term here,
而这第二个区间.

98
00:04:03,454 --> 00:04:04,199
that term is just
这个词是刚刚的

99
00:04:04,199 --> 00:04:05,947
a partial derivative with respect to
相对于偏导数

100
00:04:05,947 --> 00:04:11,188
theta one that we worked out on the previous line.
西塔.我们摸索出上一行。

101
00:04:11,188 --> 00:04:13,582
And just as a quick reminder,
而只是作为一个快速提醒.

102
00:04:13,582 --> 00:04:15,485
you must, when implementing gradient descent,
你必须实施梯度下降时.

103
00:04:15,485 --> 00:04:17,067
there's actually there's detail that, you
实际上有细节.你

104
00:04:17,067 --> 00:04:19,248
know, you should be implementing
知道了.你应该执行

105
00:04:19,248 --> 00:04:24,452
it so the update theta zero and theta one simultaneously.
所以更新THETA零和theta同时。

106
00:04:24,452 --> 00:04:28,270
So, let's see how gradient descent works.
所以.让我们来看看梯度下降是如何工作的。

107
00:04:28,270 --> 00:04:29,447
One of the issues we solved
我们解决的问题之一

108
00:04:29,447 --> 00:04:32,839
gradient descent is that it can be susceptible to local optima.
梯度下降的是.它可以容易局部最优。

109
00:04:32,839 --> 00:04:34,433
So, when I first explained gradient
所以.当我第一次解释梯度

110
00:04:34,449 --> 00:04:36,136
descent, I showed you this picture
血统.我发现你这幅画

111
00:04:36,136 --> 00:04:37,724
of it, you know, going downhill
它.你知道.下坡

112
00:04:37,724 --> 00:04:38,788
on the surface and we
的表面上.我们

113
00:04:38,788 --> 00:04:40,120
saw how, depending on where
看到了.这取决于

114
00:04:40,120 --> 00:04:42,872
you're initializing, you can end up with different local optima.
你初始化.您可以与不同的局部最优。

115
00:04:42,872 --> 00:04:44,846
You know, you can end up here or here.
你知道.你可以结束了.在这里或这里。

116
00:04:44,846 --> 00:04:46,958
But, it turns out that
但是.事实证明.

117
00:04:46,958 --> 00:04:49,025
the cost function for gradient
梯度的成本函数

118
00:04:49,025 --> 00:04:50,791
of cost function for linear regression
成本函数进行线性回归

119
00:04:50,791 --> 00:04:52,754
is always going to be
总是要

120
00:04:52,754 --> 00:04:55,305
a bow-shaped function like this.
弓形这样的功能。

121
00:04:55,305 --> 00:04:57,573
The technical term for this
技术术语

122
00:04:57,573 --> 00:05:01,163
is that this is called a convex function.
是.这是一个凸函数。

123
00:05:02,825 --> 00:05:04,920
And I'm not going
而且我不打算

124
00:05:04,920 --> 00:05:06,561
to give the formal definition for what
什么.得到正式的定义

125
00:05:06,561 --> 00:05:09,557
is a convex function, c-o-n-v-e-x, but
是一个凸函数.凸.但

126
00:05:09,557 --> 00:05:11,310
informally a convex function
非正式的凸函数

127
00:05:11,310 --> 00:05:14,793
means a bow-shaped function, you know, kind of like a bow shaped.
指弓形功能.你知道的.那种像一个弓形。

128
00:05:14,793 --> 00:05:18,006
And so, this function doesn't
所以.这个功能不

129
00:05:18,006 --> 00:05:19,738
have any local optima, except
任何局部最优.除

130
00:05:19,738 --> 00:05:22,433
for the one global optimum.
一个全局最优。

131
00:05:22,433 --> 00:05:24,265
And does gradient descent on
和梯度下降

132
00:05:24,265 --> 00:05:25,590
this type of cost function which
这种类型的成本函数

133
00:05:25,590 --> 00:05:27,695
you get whenever you're using linear
只要您是使用线性

134
00:05:27,695 --> 00:05:29,201
regression, it will always convert
回归.它总是会转换

135
00:05:29,201 --> 00:05:33,623
to the global optimum, because there are no other local optima other than global optimum.
到全局最优.因为有没有其他的本地最优以外的全局最优。

136
00:05:33,623 --> 00:05:37,272
So now, let's see this algorithm in action.
所以.现在.让我们来看看这个算法在行动。

137
00:05:38,026 --> 00:05:40,085
As usual, here are plots of
像往常一样.这里有地块

138
00:05:40,085 --> 00:05:42,182
the hypothesis function and of
设定功能和

139
00:05:42,182 --> 00:05:45,024
my cost function J.
我的成本函数J.

140
00:05:45,763 --> 00:05:47,011
And so, let's see how
所以.让我们来看看如何

141
00:05:47,011 --> 00:05:49,687
to initialize my parameters at this value.
我在这个值的参数初始化。

142
00:05:49,687 --> 00:05:51,652
You know, let's say, usually you
你知道.让我们说.通常你

143
00:05:51,652 --> 00:05:53,590
initialize your parameters at zero
初始化参数为零

144
00:05:53,590 --> 00:05:56,287
for zero, theta zero and zero.
为0.θ0和零。

145
00:05:56,287 --> 00:05:58,331
For illustration in this
如需插图

146
00:05:58,331 --> 00:05:59,948
specific presentation, I have
具体的介绍.我有

147
00:05:59,948 --> 00:06:02,615
initialised theta zero at
初始化的theta为零

148
00:06:02,615 --> 00:06:06,831
about 900, and theta one at about minus 0.1, okay?
约900.和theta一个约零下0.1.好吗？

149
00:06:06,831 --> 00:06:09,791
And so, this corresponds to H
等.这相当于为H

150
00:06:09,791 --> 00:06:12,022
over X, equals, you know,
超过X.等于.你知道.

151
00:06:12,022 --> 00:06:15,859
minus 900 minus 0.1 x
减去900减去0.1×

152
00:06:15,859 --> 00:06:19,341
is this line, so out here on the cost function.
这条线.所以这里的成本函数。

153
00:06:19,341 --> 00:06:20,491
Now if we take one
现在.如果我们采取一

154
00:06:20,491 --> 00:06:22,163
step of gradient descent, we end
梯度下降的步骤.我们最终

155
00:06:22,163 --> 00:06:24,298
up going from this point
从这个角度去

156
00:06:24,298 --> 00:06:27,065
out here, a little
在这里.一点点

157
00:06:27,065 --> 00:06:29,564
bit to the down left
位的上下左右

158
00:06:29,564 --> 00:06:31,732
to that second point over there.
那边.第二点。

159
00:06:31,732 --> 00:06:35,279
And, you notice that my line changed a little bit.
而且.你注意到我行改变了一点点。

160
00:06:35,279 --> 00:06:36,547
And, as I take another step
而且.我采取的又一步骤

161
00:06:36,547 --> 00:06:41,425
at gradient descent, my line on the left will change.
梯度下降.将改变我行左侧。

162
00:06:41,425 --> 00:06:42,376
Right.
右。

163
00:06:42,376 --> 00:06:43,804
And I have also
而且我也有

164
00:06:43,804 --> 00:06:47,544
moved to a new point on my cost function.
移动到一个新的起点上我的成本函数。

165
00:06:47,544 --> 00:06:48,745
And as I think further step
因为我觉得进一步加强

166
00:06:48,745 --> 00:06:50,697
is gradient descent, I'm going
是梯度下降.我要去

167
00:06:50,697 --> 00:06:53,058
down in cost, right, so
在成本下降.对.所以

168
00:06:53,058 --> 00:06:55,079
my parameter is following
我的参数是继

169
00:06:55,079 --> 00:06:58,062
this trajectory, and if
这个轨迹.并且如果

170
00:06:58,062 --> 00:07:00,368
you look on the left, this corresponds
在左边.你看.这相当于

171
00:07:00,368 --> 00:07:04,025
to hypotheses that seem
假设似乎

172
00:07:04,025 --> 00:07:04,912
to be getting to be
越来越成为

173
00:07:04,912 --> 00:07:06,429
better and better fits for the
更好更适合的

174
00:07:06,429 --> 00:07:09,987
data until eventually,
直到最后的数据.

175
00:07:09,987 --> 00:07:12,663
I have now wound up at the global minimum.
我现在已经卷绕在全局最小值。

176
00:07:12,663 --> 00:07:16,189
And this global minimum corresponds to
这个全球最小对应

177
00:07:16,189 --> 00:07:20,506
this hypothesis, which gives me a good fit to the data.
这一假说.这给了我一个很好的适合的数据。

178
00:07:20,922 --> 00:07:23,605
And so that's gradient
所以这是梯度

179
00:07:23,605 --> 00:07:24,969
descent, and we've just run
血统.我们刚刚运行

180
00:07:24,969 --> 00:07:27,302
it and gotten a good
并得到了良好

181
00:07:27,302 --> 00:07:31,359
fit to my data set of housing prices.
适合我的数据集的住房价格。

182
00:07:31,359 --> 00:07:34,108
And you can now use it to predict.
现在你可以用它来预测。

183
00:07:34,108 --> 00:07:35,284
You know, if your friend has a
你知道.如果你的朋友有

184
00:07:35,284 --> 00:07:36,452
house with a
房子有一个

185
00:07:36,452 --> 00:07:39,116
size 1250 square feet, you
尺寸1250平方尺.

186
00:07:39,116 --> 00:07:40,684
can now read off the value
现在可以读出的值

187
00:07:40,684 --> 00:07:42,090
and tell them that, I don't
并告诉他们.我不

188
00:07:42,090 --> 00:07:43,188
know, maybe they can get
知道.也许他们可以得到

189
00:07:43,188 --> 00:07:47,159
$350,000 for their house.
他们的房子35万美元。

190
00:07:48,606 --> 00:07:49,982
Finally, just to give
最后.只是为了给

191
00:07:49,982 --> 00:07:51,930
this another name, it turns out
这另一个名字.它的出现

192
00:07:51,930 --> 00:07:52,991
that the algorithm that we
该算法.我们

193
00:07:52,991 --> 00:07:55,030
just went over is sometimes
刚刚走过去.有时

194
00:07:55,030 --> 00:07:57,074
called batch gradient descent.
称为批处理梯度下降。

195
00:07:57,074 --> 00:07:58,781
And it turns out in machine
而事实证明.在机

196
00:07:58,781 --> 00:08:00,203
learning, I feel like us machine
学习.我觉得像我们这样的机

197
00:08:00,203 --> 00:08:02,050
learning people, we're not always
学习的人.我们并不总是

198
00:08:02,050 --> 00:08:04,381
created has given me some algorithms.
创建给了我一些算法。

199
00:08:04,381 --> 00:08:06,634
But the term batch gradient descent
但长期批次梯度下降

200
00:08:06,634 --> 00:08:08,212
means that refers to the
装置.指的是

201
00:08:08,212 --> 00:08:09,551
fact that, in every step
事实.即在每一步

202
00:08:09,551 --> 00:08:11,164
of gradient descent we're looking
我们正在寻找梯度下降

203
00:08:11,164 --> 00:08:13,649
at all of the training examples.
在所有的训​​练实例。

204
00:08:13,649 --> 00:08:15,783
So, in gradient descent, you
所以.在梯度下降.

205
00:08:15,783 --> 00:08:18,875
know, when computing derivatives, we're computing
知道.计算微分工具时.我们计算

206
00:08:18,875 --> 00:08:21,307
these sums, this sum of.
这些钱.这笔钱的。

207
00:08:21,307 --> 00:08:22,210
So, in every separate
所以.在每一个单独的

208
00:08:22,210 --> 00:08:23,510
gradient descent, we end up
梯度下降.我们最终

209
00:08:23,510 --> 00:08:25,278
computing something like this, that
计算这样的事情.那

210
00:08:25,278 --> 00:08:28,434
sums over our M training examples.
总结我们的并购训练实例。

211
00:08:28,434 --> 00:08:29,835
And so the term batch gradient
因此术语批处理梯度

212
00:08:29,835 --> 00:08:31,195
descent refers to the fact
血统是指这样的事实

213
00:08:31,195 --> 00:08:33,193
when looking at the entire batch
当看着整批

214
00:08:33,193 --> 00:08:34,559
of training examples, and again,
训练样例.并再次.

215
00:08:34,559 --> 00:08:35,742
this is really, really not
这是真的.真的不

216
00:08:35,742 --> 00:08:36,915
a great name, but this is
一个伟大的名字.但是这是

217
00:08:36,915 --> 00:08:39,444
what Machine Learning people call it.
机器学习的人称呼它。

218
00:08:39,444 --> 00:08:40,958
And it turns out there are
原来有

219
00:08:40,958 --> 00:08:42,665
sometimes other versions of
有时其他版本的

220
00:08:42,665 --> 00:08:43,918
gradient descent that are not
梯度下降不

221
00:08:43,918 --> 00:08:45,969
batch versions but instead do
批量版本.而是做

222
00:08:45,969 --> 00:08:47,752
not look at the entire traning set
不看整个教育训练集

223
00:08:47,752 --> 00:08:49,722
but look at small subsets
但看在小的子集

224
00:08:49,722 --> 00:08:51,529
of the training sets at the time,
训练集的时候.

225
00:08:51,529 --> 00:08:54,874
and we'll talk about those versions later in this course as well.
.我们会谈论这些版本在这个过程中也是如此。

226
00:08:54,874 --> 00:08:56,195
But for now, using the algorithm
但就目前而言.使用该算法

227
00:08:56,195 --> 00:08:57,410
you just learned, now we're
你刚才的教训.现在我们

228
00:08:57,410 --> 00:08:58,759
using batch gradient descent, you
使用批处理梯度下降.

229
00:08:58,759 --> 00:09:01,159
now know how to implement
现在知道如何实现

230
00:09:01,159 --> 00:09:04,149
gradient descent, or linear regression.
梯度下降.或线性回归。

231
00:09:05,856 --> 00:09:08,672
So that's linear regression with gradient descent.
所以线性回归梯度下降。

232
00:09:09,349 --> 00:09:11,747
If you've seen advanced linear algebra
如果你看过了先进的线性代数

233
00:09:11,747 --> 00:09:12,672
before so some you may
所以有些你可能之前

234
00:09:12,672 --> 00:09:14,206
have taken a class with advanced
已经采取了先进的类

235
00:09:14,206 --> 00:09:16,279
linear algebra, you might
线性代数.你可能会

236
00:09:16,295 --> 00:09:17,678
know that there exists a solution
知道存在一个解

237
00:09:17,678 --> 00:09:19,754
for numerically solving for the
为求解

238
00:09:19,754 --> 00:09:20,914
minimum of the cost function
最低的成本函数

239
00:09:20,914 --> 00:09:22,561
J, without needing to
J.而无需

240
00:09:22,561 --> 00:09:25,604
use and iterative algorithm like gradient descent.
使用像梯度下降的迭代算法。

241
00:09:25,604 --> 00:09:27,154
Later in this course we will
后来在这个过程中.我们将

242
00:09:27,154 --> 00:09:28,098
talk about that method as
谈谈该方法作为

243
00:09:28,098 --> 00:09:29,753
well that just solves for the
好.只是解决了

244
00:09:29,753 --> 00:09:31,076
minimum cost function J without
没有最低成本函数J

245
00:09:31,076 --> 00:09:33,791
needing this multiple steps of gradient descent.
需要多个步骤梯度下降。

246
00:09:33,791 --> 00:09:37,656
That other method is called normal equations methods.
其他的方法被称为正规方程组的方法。

247
00:09:37,656 --> 00:09:39,167
And, but in case you
..但如果你

248
00:09:39,167 --> 00:09:40,141
have heard of that method, it turns
原来听说过该方法.

249
00:09:40,141 --> 00:09:41,622
out gradient descent will
出梯度下降

250
00:09:41,622 --> 00:09:43,774
scale better to larger data
更好地扩展到更大的数据

251
00:09:43,774 --> 00:09:45,008
sets than that normal equals
等于比正常组

252
00:09:45,008 --> 00:09:47,315
method and, now that
方法.现在

253
00:09:47,315 --> 00:09:48,756
we know about gradient descent, we'll
我们知道梯度下降.我们会

254
00:09:48,756 --> 00:09:49,922
be able to use it in
可以使用在

255
00:09:49,922 --> 00:09:51,387
lots of different contexts, and we'll
很多不同的上下文中.我们将

256
00:09:51,387 --> 00:09:54,849
use it in lots of different Machine Learning problems as well.
在不同的机器学习问题.以及大量的使用它。

257
00:09:54,849 --> 00:09:57,138
So, congrats on learning
所以.恭喜学习

258
00:09:57,138 --> 00:10:00,317
about your first Machine Learning algorithm.
关于你的第一个机器学习算法。

259
00:10:00,317 --> 00:10:02,508
We'll later have exercises in
我们稍后将有演习

260
00:10:02,508 --> 00:10:03,659
which we'll ask you to
我们会问你

261
00:10:03,659 --> 00:10:05,068
implement gradient descent and
实施梯度下降法和

262
00:10:05,068 --> 00:10:07,071
hopefully see these algorithms work for yourselves.
希望看到这些算法为自己工作。

263
00:10:07,071 --> 00:10:08,955
But before that I first
但在此之前.我第一次

264
00:10:08,955 --> 00:10:10,587
want to tell you in
想告诉你

265
00:10:10,587 --> 00:10:11,919
the next set of videos, the
下一组视频.

266
00:10:11,919 --> 00:10:13,223
first want to tell you about
首先要告诉你

267
00:10:13,223 --> 00:10:15,724
a generalization of the gradient descent
梯度下降的泛化

268
00:10:15,724 --> 00:10:16,935
algorithm that will make
算法.这将使

269
00:10:16,935 --> 00:10:18,403
it much more powerful and I
它更强大.我

270
00:10:18,403 --> 99:59:59,000
guess I will tell you about that in the next video.
我想我会告诉你.在接下来的视频。

